{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning CoMaze Template",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyEyUoNesM52"
      },
      "source": [
        "# Installing the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svH1hL1teVFu",
        "outputId": "08129503-0918-4da1-c008-ce7971fd1dab"
      },
      "source": [
        "!git clone https://github.com/Near32/comaze-python.git ; cd comaze-python; git checkout develop-rl-template; git pull; git status; pip install -e ."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'comaze-python' already exists and is not an empty directory.\n",
            "Already on 'develop-rl-template'\n",
            "Your branch is up to date with 'origin/develop-rl-template'.\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 17 (delta 8), reused 17 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n",
            "From https://github.com/Near32/comaze-python\n",
            "   52a41aa..4117798  develop-rl-template -> origin/develop-rl-template\n",
            "Updating 52a41aa..4117798\n",
            "Fast-forward\n",
            " comaze/agents/abstract_agent.py                    |  14 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
            " comaze/agents/rl/abstract_on_policy_rl_agent.py    |  54 \u001b[32m++++++\u001b[m\u001b[31m----\u001b[m\n",
            " comaze/agents/rl/simple_on_policy_rl_agent.py      |  11 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " comaze/env/comaze.py                               |  24 \u001b[32m+++\u001b[m\u001b[31m--\u001b[m\n",
            " setup.py                                           |  17 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
            " .../rl/test_training_simple_on_policy_rl_agent.py  | 111 \u001b[32m+++++++++++++++++++++\u001b[m\n",
            " .../test_dict_encoded_pov_avail_move_exp_utils.py  |   1 \u001b[32m+\u001b[m\n",
            " 7 files changed, 184 insertions(+), 48 deletions(-)\n",
            " create mode 100644 tests/agents/rl/test_training_simple_on_policy_rl_agent.py\n",
            "On branch develop-rl-template\n",
            "Your branch is up to date with 'origin/develop-rl-template'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31mcomaze.egg-info/\u001b[m\n",
            "\t\u001b[31mcomaze/__pycache__/\u001b[m\n",
            "\t\u001b[31mcomaze/agents/__pycache__/\u001b[m\n",
            "\t\u001b[31mcomaze/agents/rl/__pycache__/\u001b[m\n",
            "\t\u001b[31mcomaze/agents/utils/__pycache__/\u001b[m\n",
            "\t\u001b[31mcomaze/env/__pycache__/\u001b[m\n",
            "\t\u001b[31mcomaze/utils/__pycache__/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "Obtaining file:///content/comaze-python\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from comaze==0.0.1) (4.41.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from comaze==0.0.1) (0.17.3)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (from comaze==0.0.1) (2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from comaze==0.0.1) (1.7.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from comaze==0.0.1) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->comaze==0.0.1) (1.19.4)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->comaze==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->comaze==0.0.1) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->comaze==0.0.1) (1.5.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx->comaze==0.0.1) (3.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx->comaze==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->comaze==0.0.1) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->comaze==0.0.1) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->comaze==0.0.1) (8.0.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->comaze==0.0.1) (0.18.2)\n",
            "Installing collected packages: comaze\n",
            "  Found existing installation: comaze 0.0.1\n",
            "    Can't uninstall 'comaze'. No files were found to uninstall.\n",
            "  Running setup.py develop for comaze\n",
            "Successfully installed comaze\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaD7n1EnsTHP"
      },
      "source": [
        "# Before continuing any further, please restart the kernel (Runtime->restart runtime) in order to make the installed packaged available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYIAV-HgoHio"
      },
      "source": [
        "# Create a simple On-Policy RL Agent:\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe95Vt3loOL6"
      },
      "source": [
        "from typing import Any\n",
        "from typing import Dict\n",
        "from typing import List\n",
        "from typing import Callable\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "import gym\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import distributions \n",
        "\n",
        "from comaze.agents.rl import AbstractOnPolicyRLAgent\n",
        "from comaze.agents.utils import dict_encoded_pov_avail_moves_extract_exp_fn, discrete_direction_only_format_move_fn\n",
        "\n",
        "\n",
        "class SimpleOnPolicyRLAgent(AbstractOnPolicyRLAgent):\n",
        "  \"\"\"\n",
        "  Simple on-policy RL agents using PyTorch.\n",
        "  \n",
        "  Call init_rl_algo at the end of the init function.\n",
        "\n",
        "  The output of select_action must be a dictionnary containing:\n",
        "    - \"action\": the actual action that needs to be transformed \n",
        "                using the format_move_fn function.\n",
        "    - \"log_prob_action\": the log likelihood over the action\n",
        "                          distribution. \n",
        "  \n",
        "  Note the default extract_exp_fn and format_move_fn functions.\n",
        "  They are the minimum to allow any learning to take place.\n",
        "\n",
        "  As AbstractAgent requests it, you also need to implement:\n",
        "    - agent_id: Agent's unique id.\n",
        "    - select_action: Agent's action selection logic.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "    self, \n",
        "    learning_rate: float=1e-4,\n",
        "    discount_factor: float=0.99,\n",
        "    num_actions: int=5,\n",
        "    pov_shape: List[int]=[7,7,12],\n",
        "    agent_order: int=0, \n",
        "    environment: Optional[gym.Env]=None, \n",
        "    ) -> None:\n",
        "    \"\"\"\n",
        "    Initializes the agent.\n",
        "    \"\"\"\n",
        "    nn.Module.__init__(self=self)\n",
        "    AbstractOnPolicyRLAgent.__init__(\n",
        "      self=self,\n",
        "      extract_exp_fn=dict_encoded_pov_avail_moves_extract_exp_fn, \n",
        "      format_move_fn=discrete_direction_only_format_move_fn,\n",
        "      learning_rate=learning_rate,\n",
        "      discount_factor=discount_factor,\n",
        "      agent_order=agent_order,\n",
        "      environment=environment,\n",
        "    )\n",
        "\n",
        "    self.num_actions = num_actions\n",
        "    self.pov_shape = pov_shape\n",
        "    self.build_agent()\n",
        "    \n",
        "    self.init_rl_algo()\n",
        "  \n",
        "  @property\n",
        "  def agent_id(self) -> str:\n",
        "    return \"simple_onpolicy_rlagent\"\n",
        "  \n",
        "  def build_agent(self):\n",
        "    self.embed_pov_size = 256\n",
        "    self.embed_pov = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=self.pov_shape[-1], out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(512, self.embed_pov_size),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "    \n",
        "    self.embed_action_size = 128\n",
        "    self.embed_action_space = nn.Linear(self.num_actions, self.embed_action_size)\n",
        "    \n",
        "    policy_input_size = self.embed_pov_size+self.embed_action_size\n",
        "    self.policy = nn.Linear(policy_input_size, self.num_actions)\n",
        "  \n",
        "  def get_formatted_inputs(self, obs):\n",
        "    nobs = {}\n",
        "    for k,v in obs.items():\n",
        "      if 'pov' in k:\n",
        "        # move channels around:\n",
        "        assert len(v.shape)==3\n",
        "        v = np.transpose(v, (2,0,1))\n",
        "      nv = torch.from_numpy(v).unsqueeze(0).float()\n",
        "      nobs[k] = nv\n",
        "    return nobs\n",
        "\n",
        "  def select_action(self, observation: Any) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Returns agent's action given `observation`.\n",
        "    \"\"\"\n",
        "\n",
        "    obs = self.get_formatted_inputs(observation)\n",
        "\n",
        "    pov_input = obs[\"encoded_pov\"]\n",
        "    action_space = obs[\"available_moves\"]\n",
        "    \n",
        "    pov_emb = self.embed_pov(pov_input)\n",
        "    action_emb = self.embed_action_space(action_space)\n",
        "    \n",
        "    pov_action_emb = torch.cat((pov_emb, action_emb), dim=1)\n",
        "    action_pred = self.policy(pov_action_emb)\n",
        "    \n",
        "    action_prob = F.softmax(action_pred, dim = -1)  \n",
        "    avail_action_prob = action_prob * obs[\"available_moves\"]\n",
        "    dist = distributions.Categorical(avail_action_prob)\n",
        "    action = dist.sample()\n",
        "    log_prob_action = dist.log_prob(action)\n",
        "\n",
        "    action_dict = {\n",
        "      \"action\": action.item(),\n",
        "      \"log_prob_action\": log_prob_action\n",
        "    }\n",
        "\n",
        "    return action_dict"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so6LR4Yqe5Mw"
      },
      "source": [
        "import random\n",
        "from typing import Callable\n",
        "import pandas as pd \n",
        "\n",
        "from functools import partial\n",
        "from tqdm import tqdm \n",
        "\n",
        "from comaze.env import TwoPlayersCoMazeGym\n",
        "from comaze.agents import AbstractAgent #, SimpleOnPolicyRLAgent\n",
        "\n",
        "\n",
        "def two_players_environment_loop(\n",
        "    agent1: AbstractAgent,\n",
        "    agent2: AbstractAgent,\n",
        "    environment,\n",
        "    max_episode_length,\n",
        "):\n",
        "  \"\"\"\n",
        "  Loop runner for the environment.\n",
        "  \"\"\"\n",
        "\n",
        "  # Setup environment.\n",
        "  #environment = TwoPlayersCoMazeGym(**environment_kwargs)\n",
        "  state = environment.reset()\n",
        "\n",
        "  # Initialize agents.\n",
        "  agent1.set_environment(environment=environment, agent_order=0)\n",
        "  agent2.set_environment(environment=environment, agent_order=1)\n",
        "\n",
        "  # Book-keeping.\n",
        "  t = 0\n",
        "  done = False\n",
        "  trajectory = list()\n",
        "\n",
        "  ebar = tqdm(total=max_episode_length, position=1)\n",
        "  while not done and t<=max_episode_length:\n",
        "    ebar.update(1)\n",
        "    # Turn-based game.\n",
        "    if t%2 == 0:\n",
        "      move = agent1.select_move(state)\n",
        "    else:\n",
        "      move = agent2.select_move(state)\n",
        "  \n",
        "    # Progress simulation.\n",
        "    next_state, reward, done, info = environment.step(move)\n",
        "\n",
        "    # Used for logging.\n",
        "    trajectory.append((t, state, move, reward, next_state, done, info))\n",
        "\n",
        "    # Agent internals.\n",
        "    \"\"\"\n",
        "    if t%2 == 0:\n",
        "      agent1.update(move, next_state, reward, done)\n",
        "    else:\n",
        "      agent2.update(move, next_state, reward, done)\n",
        "    \"\"\"\n",
        "    if t==max_episode_length:\n",
        "      done = True\n",
        "      reward = -1\n",
        "    \n",
        "    for agent in [agent1, agent2]:\n",
        "      agent.update(move, next_state, reward, done)\n",
        "\n",
        "    # Book-keeping.\n",
        "    t = t + 1\n",
        "    state = next_state\n",
        "  \n",
        "\n",
        "  # Dump logs.\n",
        "  pd.DataFrame(trajectory).to_csv(\"{}-{}.csv\".format(\n",
        "      agent1.agent_id, agent2.agent_id)\n",
        "  )\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDP6P3cJ39rd"
      },
      "source": [
        "## Let us test the agent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLO42an0fO2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad28d20b-461e-43ff-ea81-27d94bfaeb07"
      },
      "source": [
        "agent1 = SimpleOnPolicyRLAgent( \n",
        "  learning_rate=1e-4,\n",
        "  discount_factor=0.99,\n",
        "  num_actions=5,\n",
        "  pov_shape=[7,7,12],\n",
        ")\n",
        "\n",
        "agent2 = SimpleOnPolicyRLAgent( \n",
        "  learning_rate=1e-4,\n",
        "  discount_factor=0.99,\n",
        "  num_actions=5,\n",
        "  pov_shape=[7,7,12],\n",
        ")\n",
        "\n",
        "max_episode_length = 50\n",
        "verbose = False \n",
        "\n",
        "environment_kwargs = {\n",
        "    \"level\":\"1\",\n",
        "    \"verbose\":verbose,\n",
        "}\n",
        "environment = TwoPlayersCoMazeGym(**environment_kwargs)\n",
        "\n",
        "two_players_environment_loop(\n",
        "    agent1=agent1,\n",
        "    agent2=agent2,\n",
        "    environment=environment,\n",
        "    max_episode_length=max_episode_length,\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4%|▍         | 2/50 [00:00<00:02, 16.63it/s]\u001b[A\n",
            "  6%|▌         | 3/50 [00:00<00:03, 12.66it/s]\u001b[A\n",
            "  8%|▊         | 4/50 [00:00<00:04, 10.62it/s]\u001b[A\n",
            " 10%|█         | 5/50 [00:00<00:04,  9.76it/s]\u001b[A\n",
            " 12%|█▏        | 6/50 [00:00<00:04,  9.23it/s]\u001b[A\n",
            " 14%|█▍        | 7/50 [00:00<00:04,  8.81it/s]\u001b[A\n",
            " 16%|█▌        | 8/50 [00:00<00:04,  8.57it/s]\u001b[A\n",
            " 18%|█▊        | 9/50 [00:00<00:04,  8.38it/s]\u001b[A\n",
            " 20%|██        | 10/50 [00:01<00:04,  8.28it/s]\u001b[A\n",
            " 22%|██▏       | 11/50 [00:01<00:04,  8.26it/s]\u001b[A\n",
            " 24%|██▍       | 12/50 [00:01<00:04,  8.29it/s]\u001b[A\n",
            " 26%|██▌       | 13/50 [00:01<00:04,  8.32it/s]\u001b[A\n",
            " 28%|██▊       | 14/50 [00:01<00:04,  8.27it/s]\u001b[A\n",
            " 30%|███       | 15/50 [00:01<00:04,  8.25it/s]\u001b[A\n",
            " 32%|███▏      | 16/50 [00:01<00:04,  8.24it/s]\u001b[A\n",
            " 34%|███▍      | 17/50 [00:01<00:03,  8.26it/s]\u001b[A\n",
            " 36%|███▌      | 18/50 [00:02<00:03,  8.20it/s]\u001b[A\n",
            " 38%|███▊      | 19/50 [00:02<00:03,  8.20it/s]\u001b[A\n",
            " 40%|████      | 20/50 [00:02<00:03,  8.20it/s]\u001b[A\n",
            " 42%|████▏     | 21/50 [00:02<00:03,  8.22it/s]\u001b[A\n",
            " 44%|████▍     | 22/50 [00:02<00:03,  8.23it/s]\u001b[A\n",
            " 46%|████▌     | 23/50 [00:02<00:03,  8.19it/s]\u001b[A\n",
            " 48%|████▊     | 24/50 [00:02<00:03,  8.22it/s]\u001b[A\n",
            " 50%|█████     | 25/50 [00:02<00:03,  8.21it/s]\u001b[A\n",
            " 52%|█████▏    | 26/50 [00:03<00:02,  8.21it/s]\u001b[A\n",
            " 54%|█████▍    | 27/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
            " 56%|█████▌    | 28/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
            " 58%|█████▊    | 29/50 [00:03<00:02,  8.19it/s]\u001b[A\n",
            " 60%|██████    | 30/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
            " 62%|██████▏   | 31/50 [00:03<00:02,  8.23it/s]\u001b[A\n",
            " 64%|██████▍   | 32/50 [00:03<00:02,  8.29it/s]\u001b[A\n",
            " 66%|██████▌   | 33/50 [00:03<00:02,  8.31it/s]\u001b[A\n",
            " 68%|██████▊   | 34/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
            " 70%|███████   | 35/50 [00:04<00:01,  8.16it/s]\u001b[A\n",
            " 72%|███████▏  | 36/50 [00:04<00:01,  8.20it/s]\u001b[A\n",
            " 74%|███████▍  | 37/50 [00:04<00:01,  8.27it/s]\u001b[A\n",
            " 76%|███████▌  | 38/50 [00:04<00:01,  7.70it/s]\u001b[A\n",
            " 78%|███████▊  | 39/50 [00:04<00:01,  7.89it/s]\u001b[A\n",
            " 80%|████████  | 40/50 [00:04<00:01,  8.07it/s]\u001b[A\n",
            " 82%|████████▏ | 41/50 [00:05<00:03,  2.36it/s]\u001b[A\n",
            " 84%|████████▍ | 42/50 [00:07<00:05,  1.58it/s]\u001b[A\n",
            " 86%|████████▌ | 43/50 [00:07<00:03,  2.09it/s]\u001b[A\n",
            " 88%|████████▊ | 44/50 [00:07<00:02,  2.69it/s]\u001b[A\n",
            " 90%|█████████ | 45/50 [00:07<00:01,  3.37it/s]\u001b[A\n",
            " 92%|█████████▏| 46/50 [00:07<00:00,  4.11it/s]\u001b[A\n",
            " 94%|█████████▍| 47/50 [00:08<00:01,  1.97it/s]\u001b[A\n",
            " 96%|█████████▌| 48/50 [00:09<00:01,  1.45it/s]\u001b[A\n",
            " 98%|█████████▊| 49/50 [00:09<00:00,  1.89it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:10<00:00,  2.46it/s]\u001b[A\n",
            "51it [00:10,  3.12it/s]                        \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss -0.1445927619934082 :: EP reward -1\n",
            "Loss 0.11999654769897461 :: EP reward -1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IVwoOeX4Cuf"
      },
      "source": [
        "# Training a diad of SimpleOnPolicyRLAgent agents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3XqiJqKoAHK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1a43c4c-3617-497e-a83b-9f14f90e2b1f"
      },
      "source": [
        "agent1 = SimpleOnPolicyRLAgent( \n",
        "  learning_rate=1e-4,\n",
        "  discount_factor=0.99,\n",
        "  num_actions=5,\n",
        "  pov_shape=[7,7,12],\n",
        ")\n",
        "\n",
        "agent2 = SimpleOnPolicyRLAgent( \n",
        "  learning_rate=1e-4,\n",
        "  discount_factor=0.99,\n",
        "  num_actions=5,\n",
        "  pov_shape=[7,7,12],\n",
        ")\n",
        "\n",
        "max_episode_length = 50\n",
        "nbr_training_episodes = 1000\n",
        "verbose = False \n",
        "\n",
        "tbar = tqdm(total=nbr_training_episodes, position=0)\n",
        "for episode in range(nbr_training_episodes):\n",
        "  tbar.update(1)\n",
        "  environment_kwargs = {\n",
        "      \"level\":\"1\",\n",
        "      \"verbose\":verbose,\n",
        "  }\n",
        "  environment = TwoPlayersCoMazeGym(**environment_kwargs)\n",
        "\n",
        "  two_players_environment_loop(\n",
        "      agent1=agent1,\n",
        "      agent2=agent2,\n",
        "      environment=environment,\n",
        "      max_episode_length=max_episode_length,\n",
        "  )\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4%|▍         | 2/50 [00:00<00:02, 16.83it/s]\u001b[A\n",
            "  6%|▌         | 3/50 [00:00<00:03, 12.97it/s]\u001b[A\n",
            "  8%|▊         | 4/50 [00:00<00:04, 10.97it/s]\u001b[A\n",
            " 10%|█         | 5/50 [00:00<00:04,  9.83it/s]\u001b[A\n",
            " 12%|█▏        | 6/50 [00:00<00:04,  9.34it/s]\u001b[A\n",
            " 14%|█▍        | 7/50 [00:00<00:04,  8.93it/s]\u001b[A\n",
            " 16%|█▌        | 8/50 [00:00<00:04,  8.74it/s]\u001b[A\n",
            " 18%|█▊        | 9/50 [00:00<00:04,  8.63it/s]\u001b[A\n",
            " 20%|██        | 10/50 [00:01<00:04,  8.54it/s]\u001b[A\n",
            " 22%|██▏       | 11/50 [00:01<00:04,  8.38it/s]\u001b[A\n",
            " 24%|██▍       | 12/50 [00:01<00:04,  8.42it/s]\u001b[A\n",
            " 26%|██▌       | 13/50 [00:01<00:04,  8.32it/s]\u001b[A\n",
            " 28%|██▊       | 14/50 [00:01<00:04,  8.25it/s]\u001b[A\n",
            " 30%|███       | 15/50 [00:01<00:04,  8.13it/s]\u001b[A\n",
            " 32%|███▏      | 16/50 [00:01<00:04,  7.98it/s]\u001b[A\n",
            " 34%|███▍      | 17/50 [00:01<00:04,  8.04it/s]\u001b[A\n",
            " 36%|███▌      | 18/50 [00:02<00:03,  8.17it/s]\u001b[A\n",
            " 38%|███▊      | 19/50 [00:02<00:03,  8.23it/s]\u001b[A\n",
            " 40%|████      | 20/50 [00:02<00:03,  8.26it/s]\u001b[A\n",
            " 42%|████▏     | 21/50 [00:02<00:03,  8.20it/s]\u001b[A\n",
            " 44%|████▍     | 22/50 [00:02<00:03,  8.23it/s]\u001b[A\n",
            " 46%|████▌     | 23/50 [00:02<00:03,  8.19it/s]\u001b[A\n",
            " 48%|████▊     | 24/50 [00:02<00:03,  8.19it/s]\u001b[A\n",
            " 50%|█████     | 25/50 [00:02<00:03,  8.15it/s]\u001b[A\n",
            " 52%|█████▏    | 26/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
            " 54%|█████▍    | 27/50 [00:03<00:02,  8.22it/s]\u001b[A\n",
            " 56%|█████▌    | 28/50 [00:03<00:02,  8.20it/s]\u001b[A\n",
            " 58%|█████▊    | 29/50 [00:03<00:02,  8.19it/s]\u001b[A\n",
            " 60%|██████    | 30/50 [00:03<00:02,  8.22it/s]\u001b[A\n",
            " 62%|██████▏   | 31/50 [00:03<00:02,  8.24it/s]\u001b[A\n",
            " 64%|██████▍   | 32/50 [00:03<00:02,  8.29it/s]\u001b[A\n",
            " 66%|██████▌   | 33/50 [00:03<00:02,  8.32it/s]\u001b[A\n",
            " 68%|██████▊   | 34/50 [00:04<00:01,  8.29it/s]\u001b[A\n",
            " 70%|███████   | 35/50 [00:04<00:01,  8.29it/s]\u001b[A\n",
            " 72%|███████▏  | 36/50 [00:04<00:01,  8.26it/s]\u001b[A\n",
            " 74%|███████▍  | 37/50 [00:04<00:01,  8.22it/s]\u001b[A\n",
            " 76%|███████▌  | 38/50 [00:04<00:01,  8.29it/s]\u001b[A\n",
            " 78%|███████▊  | 39/50 [00:04<00:01,  8.31it/s]\u001b[A\n",
            " 80%|████████  | 40/50 [00:04<00:01,  8.30it/s]\u001b[A\n",
            " 82%|████████▏ | 41/50 [00:04<00:01,  8.29it/s]\u001b[A\n",
            " 84%|████████▍ | 42/50 [00:05<00:01,  7.70it/s]\u001b[A\n",
            " 86%|████████▌ | 43/50 [00:05<00:00,  7.81it/s]\u001b[A\n",
            " 88%|████████▊ | 44/50 [00:05<00:00,  7.89it/s]\u001b[A\n",
            " 90%|█████████ | 45/50 [00:05<00:00,  7.94it/s]\u001b[A\n",
            " 92%|█████████▏| 46/50 [00:05<00:00,  8.01it/s]\u001b[A\n",
            " 94%|█████████▍| 47/50 [00:06<00:01,  2.36it/s]\u001b[A\n",
            " 96%|█████████▌| 48/50 [00:07<00:01,  1.58it/s]\u001b[A\n",
            " 98%|█████████▊| 49/50 [00:07<00:00,  2.08it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:08<00:00,  2.68it/s]\u001b[A\n",
            "  0%|          | 2/1000 [00:08<1:11:11,  4.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss 0.6091338992118835 :: EP reward -1\n",
            "Loss 0.20674264430999756 :: EP reward -1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4%|▍         | 2/50 [00:00<00:02, 17.52it/s]\u001b[A\n",
            "  6%|▌         | 3/50 [00:00<00:03, 13.08it/s]\u001b[A\n",
            "  8%|▊         | 4/50 [00:00<00:04, 11.20it/s]\u001b[A\n",
            " 10%|█         | 5/50 [00:00<00:04, 10.16it/s]\u001b[A\n",
            " 12%|█▏        | 6/50 [00:00<00:04,  9.13it/s]\u001b[A\n",
            " 14%|█▍        | 7/50 [00:00<00:04,  8.83it/s]\u001b[A\n",
            " 16%|█▌        | 8/50 [00:00<00:04,  8.66it/s]\u001b[A\n",
            " 18%|█▊        | 9/50 [00:00<00:04,  8.54it/s]\u001b[A\n",
            " 20%|██        | 10/50 [00:01<00:04,  8.42it/s]\u001b[A\n",
            " 22%|██▏       | 11/50 [00:01<00:04,  8.39it/s]\u001b[A\n",
            " 24%|██▍       | 12/50 [00:01<00:04,  8.24it/s]\u001b[A\n",
            " 26%|██▌       | 13/50 [00:01<00:04,  7.64it/s]\u001b[A\n",
            " 28%|██▊       | 14/50 [00:01<00:04,  7.80it/s]\u001b[A\n",
            " 30%|███       | 15/50 [00:01<00:04,  7.95it/s]\u001b[A\n",
            " 32%|███▏      | 16/50 [00:01<00:04,  8.05it/s]\u001b[A\n",
            " 34%|███▍      | 17/50 [00:01<00:04,  8.15it/s]\u001b[A\n",
            " 36%|███▌      | 18/50 [00:02<00:03,  8.10it/s]\u001b[A\n",
            " 38%|███▊      | 19/50 [00:02<00:03,  8.19it/s]\u001b[A\n",
            " 40%|████      | 20/50 [00:03<00:12,  2.37it/s]\u001b[A\n",
            " 42%|████▏     | 21/50 [00:04<00:18,  1.58it/s]\u001b[A\n",
            " 44%|████▍     | 22/50 [00:05<00:21,  1.28it/s]\u001b[A\n",
            " 46%|████▌     | 23/50 [00:06<00:23,  1.13it/s]\u001b[A\n",
            " 48%|████▊     | 24/50 [00:07<00:25,  1.04it/s]\u001b[A\n",
            " 50%|█████     | 25/50 [00:08<00:25,  1.01s/it]\u001b[A\n",
            " 52%|█████▏    | 26/50 [00:10<00:25,  1.05s/it]\u001b[A\n",
            " 54%|█████▍    | 27/50 [00:11<00:24,  1.07s/it]\u001b[A\n",
            " 56%|█████▌    | 28/50 [00:11<00:17,  1.28it/s]\u001b[A\n",
            " 58%|█████▊    | 29/50 [00:11<00:12,  1.71it/s]\u001b[A\n",
            " 60%|██████    | 30/50 [00:11<00:09,  2.20it/s]\u001b[A\n",
            " 62%|██████▏   | 31/50 [00:11<00:06,  2.81it/s]\u001b[A\n",
            " 64%|██████▍   | 32/50 [00:11<00:05,  3.51it/s]\u001b[A\n",
            " 66%|██████▌   | 33/50 [00:11<00:04,  4.23it/s]\u001b[A\n",
            " 68%|██████▊   | 34/50 [00:12<00:03,  4.94it/s]\u001b[A\n",
            " 70%|███████   | 35/50 [00:12<00:02,  5.61it/s]\u001b[A\n",
            " 72%|███████▏  | 36/50 [00:12<00:02,  6.22it/s]\u001b[A\n",
            " 74%|███████▍  | 37/50 [00:12<00:01,  6.76it/s]\u001b[A\n",
            " 76%|███████▌  | 38/50 [00:13<00:05,  2.28it/s]\u001b[A\n",
            " 78%|███████▊  | 39/50 [00:14<00:07,  1.55it/s]\u001b[A\n",
            " 80%|████████  | 40/50 [00:15<00:07,  1.27it/s]\u001b[A\n",
            " 82%|████████▏ | 41/50 [00:16<00:07,  1.13it/s]\u001b[A\n",
            " 84%|████████▍ | 42/50 [00:18<00:07,  1.04it/s]\u001b[A\n",
            " 86%|████████▌ | 43/50 [00:19<00:07,  1.01s/it]\u001b[A\n",
            " 88%|████████▊ | 44/50 [00:19<00:04,  1.34it/s]\u001b[A\n",
            " 90%|█████████ | 45/50 [00:19<00:02,  1.77it/s]\u001b[A\n",
            " 92%|█████████▏| 46/50 [00:20<00:02,  1.37it/s]\u001b[A\n",
            " 94%|█████████▍| 47/50 [00:21<00:02,  1.18it/s]\u001b[A\n",
            " 96%|█████████▌| 48/50 [00:22<00:01,  1.07it/s]\u001b[A\n",
            " 98%|█████████▊| 49/50 [00:23<00:00,  1.01it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:24<00:00,  1.37it/s]\u001b[A\n",
            "51it [00:24,  1.82it/s]                        \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss -0.6764953136444092 :: EP reward -1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7abe07d1bcb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0magent2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m       \u001b[0mmax_episode_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_episode_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   )\n",
            "\u001b[0;32m<ipython-input-5-3eb78fe83762>\u001b[0m in \u001b[0;36mtwo_players_environment_loop\u001b[0;34m(agent1, agent2, environment, max_episode_length)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m       \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Book-keeping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/comaze-python/comaze/agents/rl/abstract_on_policy_rl_agent.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, last_action, new_observation, reward, done)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/comaze-python/comaze/agents/rl/abstract_on_policy_rl_agent.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HBhpKTQr2SB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}